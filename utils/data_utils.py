"""
data_utils.py - Data processing utilities for financial time series

Contains functions for handling missing data with appropriate fill methods
based on feature types.
"""

import os
import pathlib
import requests
import time
import urllib.parse
from dotenv import load_dotenv
import pandas as pd
import numpy as np
from typing import List, Dict, Any
from pandas_market_calendars import date_range, get_calendar
from tqdm import tqdm
from datetime import datetime
from utils.env import DATA_KEY, DATA_SECRET

def load_failed_symbols(failed_file_path):
    """Load stock symbols from failed symbols file"""
    symbols = []
    try:
        with open(failed_file_path, "r") as f:
            lines = f.readlines()
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith("#"):
                symbols.append(line)
        
        return symbols
    except FileNotFoundError:
        return []
    except Exception as e:
        return []

def get_sp500_symbols():
    """Fetch S&P 500 symbols from Wikipedia"""
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    return pd.read_html(url)[0]["Symbol"].tolist()

def setup_config():
    """Setup configuration and constants"""
    config = {
        'headers': {
            "accept": "application/json",
            "APCA-API-KEY-ID": DATA_KEY,
            "APCA-API-SECRET-KEY": DATA_SECRET,
        },
        'base_url': "https://data.alpaca.markets/v2/stocks",
        'start_iso': "2025-01-02",
        'end_iso': "2025-07-18",
        'limit': 10_000,
        'feed_ticks': "iex",
        'out_dir': pathlib.Path("data")
    }
    
    config['out_dir'].mkdir(exist_ok=True)
    return config

def fetch_pages(url_base, data_key, headers):
    """Generic pagination fetcher for Alpaca API"""
    rows, token = [], None
    syms = url_base.split("stocks/")[1].split("/")[0]
    
    with tqdm(desc=f"Fetching {data_key} for {syms}", unit="page", leave=False) as pbar:
        while True:
            url = url_base + (f"&page_token={urllib.parse.quote_plus(token)}" if token else "")
            r = requests.get(url, headers=headers, timeout=30)
            r.raise_for_status()
            j = r.json()
            
            # Response structure can be either a list (single symbol endpoint) or a
            # dict keyed by symbol (multi-symbol endpoint). Handle both cases.
            data = j.get(data_key, {})
            if isinstance(data, dict):
                for sym, records in data.items():
                    # Attach symbol to each record to preserve identity before flattening
                    for rec in records:
                        rec["symbol"] = sym
                    rows.extend(records)
            elif isinstance(data, list):
                # Single-symbol request already includes symbol in each record (v2/stocks/trades/<symbol>)
                rows.extend(data)
            else:
                # Unexpected format â€“ skip gracefully
                pass

            token = j.get("next_page_token")
            if not token: break
            time.sleep(0.01)
            pbar.update(1)
    
    result = pd.DataFrame(rows)
    # For rare cases where symbol is still missing (e.g., malformed response), back-fill with query symbols
    if not result.empty and "symbol" not in result.columns:
        result["symbol"] = syms
    return result
    

def floor_minute(df):
    """Convert timestamps to minute floor"""
    if df.empty: return df
    # Use ISO8601 format to handle mixed timestamp formats
    df["timestamp"] = pd.to_datetime(df["t"], format='ISO8601', utc=True).dt.floor("min").dt.tz_localize(None)
    return df





def trades_to_min(tr):
    """Process trades data to minute-level aggregations with advanced microstructure features"""
    if tr.empty or not all(col in tr.columns for col in ['t', 'p', 's', 'c']):
        return pd.DataFrame() # Return empty DataFrame if essential columns are missing
    
    with tqdm(desc="Processing trades", total=10, leave=False) as pbar:
        pbar.set_description("Creating basic trade features")
        # Basic derived columns
        tr["dollar_value"] = tr["p"] * tr["s"]
        tr["cond_is_regular"] = tr["c"].apply(lambda x: int("@" in x))
        tr["odd_lot"] = tr["c"].apply(lambda x: int("I" in x))
        
        # Size classification
        def bucket_size(x):
            if x < 100:  return "tiny"
            if x < 500:  return "small"
            if x < 2000: return "mid"
            return "large"
        tr["size_bucket"] = tr["s"].map(bucket_size)
        pbar.update(1)
        
        pbar.set_description("Implementing tick rule & order flow")
        # Advanced Feature 1: Tick Rule (Lee-Ready Algorithm)
        # Classify trades as buy (+1) or sell (-1) based on price movement
        tr["price_change"] = tr.groupby("symbol")["p"].diff()
        tr["tick_rule"] = np.where(tr["price_change"] > 0, 1,
                         np.where(tr["price_change"] < 0, -1, 0))
        
        # Forward-fill undetermined trades with previous classification
        tr["tick_rule"] = tr.groupby("symbol")["tick_rule"].ffill().fillna(0)
        
        # Order flow imbalance (buy volume - sell volume)
        tr["signed_volume"] = tr["tick_rule"] * tr["s"]
        tr["signed_dollar_volume"] = tr["tick_rule"] * tr["dollar_value"]
        pbar.update(1)
        
        pbar.set_description("Calculating price impact measures")
        # Advanced Feature 2: Kyle's Lambda (Price Impact)
        # Measures how much prices move per unit of order flow
        tr["abs_price_change"] = tr["price_change"].abs()
        tr["kyle_lambda_numerator"] = tr["abs_price_change"] * tr["s"]
        pbar.update(1)
        
        pbar.set_description("Computing price acceleration features")
        # Advanced Feature 3: Price Acceleration & Trade Clustering
        tr["lag_price"] = tr.groupby("symbol")["p"].shift(1)
        tr["lag2_price"] = tr.groupby("symbol")["p"].shift(2)
        
        # Price acceleration (second derivative)
        tr["price_acceleration"] = tr["p"] - 2*tr["lag_price"] + tr["lag2_price"]
        
        # Trade clustering: consecutive trades in same direction
        tr["consecutive_direction"] = (tr["tick_rule"] == tr.groupby("symbol")["tick_rule"].shift(1)).astype(int)
        
        # Volume-weighted price momentum
        tr["volume_weighted_momentum"] = tr["tick_rule"] * tr["s"] * tr["price_change"].fillna(0)
        pbar.update(1)
        
        pbar.set_description("Aggregating price data")
        # Price aggregation - preserve OHLC data
        minute_price = (
            tr.groupby(["symbol","timestamp"])["p"]
              .agg(open="first", close="last", high="max", low="min",
                   mean="mean", std="std")
        )
        pbar.update(1)
        
        pbar.set_description("Aggregating advanced microstructure features")
        # Advanced aggregations
        minute_advanced = (
            tr.groupby(["symbol","timestamp"])
              .agg(
                  # Basic aggregations
                  trade_count=("p","count"),
                  volume=("s","sum"),
                  dollar_volume=("dollar_value","sum"),
                  cond_is_regular=("cond_is_regular","sum"),
                  odd_lot_count=("odd_lot","sum"),
                  
                  # Order flow features
                  buy_volume=("signed_volume", lambda x: x[x > 0].sum()),
                  sell_volume=("signed_volume", lambda x: -x[x < 0].sum()),
                  order_flow_imbalance=("signed_volume", "sum"),
                  dollar_order_flow_imbalance=("signed_dollar_volume", "sum"),
                  
                  # Price impact & acceleration measures
                  kyle_lambda=("kyle_lambda_numerator", "sum"),
                  avg_price_acceleration=("price_acceleration", "mean"),
                  price_acceleration_std=("price_acceleration", "std"),
                  consecutive_trades_ratio=("consecutive_direction", "mean"),
                  volume_weighted_momentum=("volume_weighted_momentum", "sum"),
                  
                  # Trade characteristics
                  avg_trade_size=("s", "mean"),
                  trade_size_std=("s", "std"),
                  max_trade_size=("s", "max"),
                  
                  # Aggressiveness measures
                  buy_trade_count=("tick_rule", lambda x: (x > 0).sum()),
                  sell_trade_count=("tick_rule", lambda x: (x < 0).sum()),
              )
        )
        pbar.update(1)
        
        pbar.set_description("Calculating time intervals")
        # Inter-trade time intervals
        tr["t_original"] = pd.to_datetime(tr["t"], format='ISO8601', utc=True)
        tr["delta_ms"] = tr.groupby("symbol")["t_original"].diff().dt.total_seconds()*1e3
        inter_ms = (tr.groupby(["symbol","timestamp"])["delta_ms"]
                      .agg(intertrade_ms_mean="mean",
                           intertrade_ms_std="std",
                           intertrade_ms_min="min",
                           intertrade_ms_max="max").fillna(0))
        pbar.update(1)
        
        pbar.set_description("Creating size bucket features")
        # Size-bucket one-hot encoding
        bucket = (tr.pivot_table(index=["symbol","timestamp"],
                                 columns="size_bucket", values="s",
                                 aggfunc="count").fillna(0)
                    .add_prefix("cnt_"))
        pbar.update(1)
        
        pbar.set_description("Computing final microstructure features")
        # Merge all features
        tm = (minute_price.join(minute_advanced)
              .join(inter_ms)
              .join(bucket)
              .reset_index())
        
        # Calculate VWAP
        tm["vwap"] = tm["dollar_volume"] / tm["volume"].replace(0, np.nan)
        
        # Advanced Feature 4: Order Flow Imbalance Ratio
        tm["order_flow_ratio"] = tm["order_flow_imbalance"] / tm["volume"].replace(0, np.nan)
        tm["dollar_order_flow_ratio"] = tm["dollar_order_flow_imbalance"] / tm["dollar_volume"].replace(0, np.nan)
        
        # Advanced Feature 5: Buy/Sell Pressure
        tm["buy_sell_ratio"] = (tm["buy_volume"] / tm["sell_volume"].replace(0, np.nan)).replace([np.inf, -np.inf], np.nan)
        tm["trade_direction_ratio"] = (tm["buy_trade_count"] - tm["sell_trade_count"]) / tm["trade_count"].replace(0, np.nan)
        
        # Advanced Feature 6: Price Impact per Dollar (Kyle's Lambda normalized)
        tm["price_impact_per_dollar"] = tm["kyle_lambda"] / tm["dollar_volume"].replace(0, np.nan)
        
        # Advanced Feature 7: Volatility and momentum features
        tm["trade_return"] = tm.groupby("symbol")["close"].pct_change()
        tm["volatility_proxy"] = tm["std"] / tm["mean"].replace(0, np.nan)  # Coefficient of variation
        tm["price_range"] = (tm["high"] - tm["low"]) / tm["open"].replace(0, np.nan)
        tm["mid_bar_pos"] = (tm["close"]-tm["low"])/(tm["high"]-tm["low"]).replace(0,np.nan)
        
        # Advanced Feature 8: Liquidity measures
        tm["turnover_rate"] = tm["volume"] / tm["avg_trade_size"].replace(0, np.nan)  # How many "typical" trades occurred
        tm["trade_intensity"] = tm["trade_count"] / (tm["intertrade_ms_mean"].replace(0, np.nan) / 1000)  # Trades per second
        
        # Advanced Feature 9: Roll's implicit spread estimate
        # Roll's measure: 2 * sqrt(-Cov(r_t, r_{t-1})) where r_t is returns
        tm["roll_spread_proxy"] = np.abs(tm["trade_return"] * tm.groupby("symbol")["trade_return"].shift(1))
        
        # Advanced Feature 10: Short-term reversal measure
        tm["price_reversal"] = (tm["close"] - tm["open"]) * (tm.groupby("symbol")["close"].shift(1) - tm.groupby("symbol")["open"].shift(1))
        
        pbar.update(1)
    
    return tm


def smart_fill_features(df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:
    """
    Apply appropriate fill methods for different types of financial features.
    
    This function handles missing data by applying different strategies based on
    the semantic meaning of each feature type:
    - Price features: Fill with last close price (market reality during gaps)
    - Volume/activity features: Fill with 0 (no activity = zero)
    - Statistical features: Special values (std=0, intertrade_ms=60000ms)
    
    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame with financial features
    verbose : bool, default False
        If True, print detailed information about the filling process
        
    Returns
    -------
    pd.DataFrame
        DataFrame with missing values filled appropriately
    """
    df = df.copy()
    
    # Define feature categories
    feature_categories = {
        'price': {
            'columns': ['open', 'high', 'low', 'close', 'vwap', 'mean',
                       'trade_return', 'price_range', 'mid_bar_pos', 
                       'roll_spread_proxy', 'price_reversal'],
            'method': 'close_fill',
            'description': 'Price-derived features (fill with last close)'
        },
        'volume': {
            'columns': ['volume', 'trade_count', 'dollar_volume', 'cond_is_regular', 
                       'odd_lot_count', 'cnt_tiny', 'cnt_small', 'cnt_mid', 'cnt_large',
                       'buy_volume', 'sell_volume', 'order_flow_imbalance', 
                       'dollar_order_flow_imbalance', 'kyle_lambda', 'volume_weighted_momentum',
                       'buy_trade_count', 'sell_trade_count'],
            'method': 'zero',
            'description': 'Volume/activity features (fill with 0)'
        },
        'ratios_and_stats': {
            'columns': ['std', 'avg_price_acceleration', 'price_acceleration_std', 
                       'consecutive_trades_ratio', 'avg_trade_size', 'trade_size_std', 
                       'max_trade_size', 'order_flow_ratio', 'dollar_order_flow_ratio',
                       'buy_sell_ratio', 'trade_direction_ratio', 'price_impact_per_dollar',
                       'volatility_proxy', 'turnover_rate', 'trade_intensity'],
            'method': 'zero',
            'description': 'Ratios and statistical measures (fill with 0)'
        },
        'time_intervals': {
            'columns': ['intertrade_ms_mean', 'intertrade_ms_std', 'intertrade_ms_min', 'intertrade_ms_max'],
            'method': 'special',
            'description': 'Time interval features (special fill)',
            'fill_values': {
                'intertrade_ms_mean': 60000,  # 1 minute default
                'intertrade_ms_std': 0,       # No variation when no trades
                'intertrade_ms_min': 60000,   # 1 minute default
                'intertrade_ms_max': 60000    # 1 minute default
            }
        }
    }
    
    # Apply fill methods by category
    for category, config in feature_categories.items():
        cols_in_df = [col for col in config['columns'] if col in df.columns]
        
        if not cols_in_df:
            continue
            
        for col in cols_in_df:
            nan_count_before = df[col].isna().sum()
            
            if config['method'] == 'ffill':
                df[col] = df[col].ffill()
            elif config['method'] == 'zero':
                df[col] = df[col].fillna(0)
            elif config['method'] == 'close_fill':
                # Fill all price columns with the last available close price
                last_close = df['close'].ffill()
                df[col] = df[col].fillna(last_close)
            elif config['method'] == 'special':
                # Use specific fill values for each column
                fill_values = config.get('fill_values', {})
                fill_value = fill_values.get(col, 0)  # Default to 0 if not specified
                df[col] = df[col].fillna(fill_value)
                
    # Final pass: any remaining NaNs get forward-filled, then back-filled
    remaining_nans_before = df.isna().sum().sum()
    if remaining_nans_before > 0:
        df = df.ffill().bfill()
    
    return df


def add_minute_norm(df: pd.DataFrame) -> pd.DataFrame:
    """
    Adds a 'minute_norm' column to the DataFrame, normalized to [0, 1] for a regular US trading session.
    Assumes 'timestamp' column is present and in datetime format or convertible.
    """
    if "minute_norm" not in df.columns and "timestamp" in df.columns:
        ts = pd.to_datetime(df["timestamp"])
        minute_of_day = ts.dt.hour * 60 + ts.dt.minute - (9 * 60 + 30)
        minute_of_day = minute_of_day.clip(lower=0, upper=389)
        df = df.copy()
        df["minute_norm"] = minute_of_day / 390.0
    return df


def align_to_nyse_timeline(df: pd.DataFrame, start_date: str, end_date: str, verbose: bool = False) -> pd.DataFrame:
    """
    Align DataFrame to complete NYSE trading timeline for specified date range.
    
    This function:
    1. Creates a canonical NYSE trading minute timeline for the specified period
    2. Aligns the data to this complete timeline using reindex
    3. Converts timestamps to proper UTC timezone-naive format
    
    Parameters
    ----------
    df : pd.DataFrame
        Input DataFrame with 'timestamp' column
    start_date : str
        Start date in ISO format (e.g., "2025-01-02")
    end_date : str
        End date in ISO format (e.g., "2025-07-01")
    verbose : bool, default False
        If True, print alignment statistics
        
    Returns
    -------
    pd.DataFrame
        DataFrame aligned to complete NYSE trading timeline
    """
    if df.empty or 'timestamp' not in df.columns:
        return df
    
    # Create canonical NYSE trading timeline for specified period
    nyse = get_calendar("NYSE")
    sched = nyse.schedule(start_date=start_date, end_date=end_date)
    canon_idx = date_range(sched, frequency="1min", closed="both")
    canon_idx = canon_idx.tz_convert("UTC").tz_localize(None)
    
    # Align data to canonical timeline
    aligned_df = (
        df.set_index("timestamp")
          .reindex(canon_idx)
          .reset_index()
          .rename(columns={"index": "timestamp"})
    )
    
    return aligned_df


def validate_fill_results(df: pd.DataFrame, symbol: str = "Unknown") -> Dict[str, Any]:
    """
    Validate the results of the fill operation and return statistics.
    
    Parameters
    ----------
    df : pd.DataFrame
        Filled DataFrame to validate
    symbol : str, default "Unknown"
        Symbol name for reporting
        
    Returns
    -------
    Dict[str, Any]
        Dictionary containing validation statistics
    """
    stats = {
        'symbol': symbol,
        'total_rows': len(df),
        'total_nans': df.isna().sum().sum(),
        'columns_with_nans': df.columns[df.isna().any()].tolist(),
        'zero_volume_pct': 0.0,
        'price_continuity_pct': 0.0
    }
    
    # Check volume features
    if 'volume' in df.columns:
        zero_volume_count = (df['volume'] == 0).sum()
        stats['zero_volume_pct'] = (zero_volume_count / len(df)) * 100
    
    # Check price continuity (consecutive identical values might indicate fills)
    if 'close' in df.columns:
        consecutive_prices = (df['close'] == df['close'].shift(1)).sum()
        stats['price_continuity_pct'] = (consecutive_prices / len(df)) * 100
    
    return stats


def align_and_clean_data(data_dict: Dict[str, pd.DataFrame], verbose: bool = False) -> Dict[str, pd.DataFrame]:
    """
    Align all stocks to a common timeline and remove timestamps where any stock has unfillable NaNs.
    
    This function:
    1. Finds the common timestamp range across all stocks
    2. Identifies timestamps where any stock has NaNs that can't be forward-filled
    3. Removes those timestamps from all stocks to maintain alignment
    
    Parameters
    ----------
    data_dict : Dict[str, pd.DataFrame]
        Dictionary mapping symbol names to DataFrames
    verbose : bool, default False
        If True, print progress and statistics
        
    Returns
    -------
    Dict[str, pd.DataFrame]
        Dictionary with aligned and cleaned DataFrames
    """
    if not data_dict:
        return {}
    
    # Find common timestamp range
    all_timestamps = set()
    for symbol, df in data_dict.items():
        all_timestamps.update(df['timestamp'].values)
    
    common_timestamps = sorted(all_timestamps)
    
    # For each stock, identify which timestamps have unfillable NaNs
    unfillable_timestamps = set()
    
    for symbol, df in data_dict.items():
        df_sorted = df.sort_values('timestamp').reset_index(drop=True)
        
        # Check if close price (our fill source) has NaNs at the beginning
        if 'close' in df_sorted.columns:
            # Find first valid close price
            first_valid_idx = df_sorted['close'].first_valid_index()
            
            if first_valid_idx is not None and first_valid_idx > 0:
                # Mark timestamps before first valid close as unfillable
                unfillable_ts = df_sorted.loc[:first_valid_idx-1, 'timestamp'].values
                unfillable_timestamps.update(unfillable_ts)
    
    # Remove unfillable timestamps from all stocks
    cleaned_dict = {}
    
    for symbol, df in data_dict.items():
        # Filter out unfillable timestamps
        mask = ~df['timestamp'].isin(unfillable_timestamps)
        cleaned_df = df[mask].copy()
        
        cleaned_dict[symbol] = cleaned_df
    
    return cleaned_dict


def apply_smart_fill_to_dict(data_dict: Dict[str, pd.DataFrame], align_stocks: bool = True, verbose: bool = False) -> Dict[str, pd.DataFrame]:
    """
    Apply smart fill to a dictionary of DataFrames (multiple symbols).
    
    Parameters
    ----------
    data_dict : Dict[str, pd.DataFrame]
        Dictionary mapping symbol names to DataFrames
    align_stocks : bool, default True
        If True, align stocks and remove unfillable timestamps first
    verbose : bool, default False
        If True, print progress and statistics
        
    Returns
    -------
    Dict[str, pd.DataFrame]
        Dictionary with filled DataFrames
    """
    if align_stocks:
        data_dict = align_and_clean_data(data_dict, verbose=verbose)
    
    filled_dict = {}
    
    for symbol, df in data_dict.items():
        filled_df = smart_fill_features(df, verbose=verbose)
        filled_dict[symbol] = filled_df
        
    return filled_dict 